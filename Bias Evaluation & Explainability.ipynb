{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_squared_error, roc_curve, auc\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shap\n",
    "from keras.layers import Input, Dense, Flatten, \\\n",
    "    Concatenate, concatenate, Dropout, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras\n",
    "from livelossplot import PlotLossesKeras\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "import scipy\n",
    "from scipy.cluster import hierarchy as hc\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import math\n",
    "\n",
    "params = {\"ytick.color\" : \"w\",\n",
    "          \"xtick.color\" : \"w\",\n",
    "          \"text.color\": \"gray\",\n",
    "          \"axes.labelcolor\" : \"w\",\n",
    "          \"axes.edgecolor\" : \"w\"}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "label_column = \"loan\"\n",
    "csv_path = 'data/adult.data'\n",
    "csv_columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "                   \"occupation\", \"relationship\", \"ethnicity\", \"gender\", \"capital-gain\", \"capital-loss\",\n",
    "                   \"hours-per-week\", \"native-country\", \"loan\"]\n",
    "input_columns = [\"age\", \"workclass\", \"education\", \"education-num\", \"marital-status\",\n",
    "                   \"occupation\", \"relationship\", \"ethnicity\", \"gender\", \"capital-gain\", \"capital-loss\",\n",
    "                   \"hours-per-week\", \"native-country\"]\n",
    "categorical_features = [\"workclass\", \"education\", \"marital-status\",\n",
    "                       \"occupation\", \"relationship\", \"ethnicity\", \"gender\",\n",
    "                       \"native-country\"]\n",
    "\n",
    "def prepare_data(df):\n",
    "    \n",
    "    if \"fnlwgt\" in df: del df[\"fnlwgt\"]\n",
    "    \n",
    "    tmp_df = df.copy()\n",
    "\n",
    "    # normalize data (this is important for model convergence)\n",
    "    dtypes = list(zip(tmp_df.dtypes.index, map(str, tmp_df.dtypes)))\n",
    "    for k,dtype in dtypes:\n",
    "        if dtype == \"int64\":\n",
    "            tmp_df[k] = tmp_df[k].astype(np.float32)\n",
    "            tmp_df[k] -= tmp_df[k].mean()\n",
    "            tmp_df[k] /= tmp_df[k].std()\n",
    "\n",
    "    cat_columns = tmp_df.select_dtypes(['object']).columns\n",
    "    tmp_df[cat_columns] = tmp_df[cat_columns].astype('category')\n",
    "    tmp_df[cat_columns] = tmp_df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    tmp_df[cat_columns] = tmp_df[cat_columns].astype('int8')\n",
    "    \n",
    "    return tmp_df\n",
    "\n",
    "def get_dataset_1():\n",
    "    tmp_df = df.copy()\n",
    "    tmp_df = tmp_df.groupby('loan') \\\n",
    "                .apply(lambda x: x.sample(100) if x[\"loan\"].iloc[0] else x) \\\n",
    "                .reset_index(drop=True)\n",
    "    \n",
    "    X = tmp_df.drop(label_column, axis=1).copy()\n",
    "    y = tmp_df[label_column].astype(int).values.copy()\n",
    "    \n",
    "    return tmp_df, df_display.copy()\n",
    "\n",
    "def get_production_dataset():\n",
    "    tmp_df = df.copy()\n",
    "    \n",
    "    X = tmp_df.drop(label_column, axis=1).copy()\n",
    "    y = tmp_df[label_column].astype(int).values.copy()\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = \\\n",
    "        train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "\n",
    "    return X_valid, y_valid\n",
    "\n",
    "\n",
    "def get_dataset_2():\n",
    "    tmp_df = df.copy()\n",
    "    tmp_df_display = df_display.copy()\n",
    "#     tmp_df_display[label_column] = tmp_df_display[label_column].astype(int).values\n",
    "    \n",
    "    X = tmp_df.drop(label_column, axis=1).copy()\n",
    "    y = tmp_df[label_column].astype(int).values.copy()\n",
    "    \n",
    "    X_display = tmp_df_display.drop(label_column, axis=1).copy()\n",
    "    y_display = tmp_df_display[label_column].astype(int).values.copy()\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = \\\n",
    "        train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "\n",
    "    return X, y, X_train, X_valid, y_train, y_valid, X_display, y_display, tmp_df, tmp_df_display\n",
    "    \n",
    "df_display = pd.read_csv(csv_path, names=csv_columns)\n",
    "df_display[label_column] = df_display[label_column].apply(lambda x: \">50K\" in x)\n",
    "df = prepare_data(df_display)\n",
    "\n",
    "def build_model(X):\n",
    "    input_els = []\n",
    "    encoded_els = []\n",
    "    dtypes = list(zip(X.dtypes.index, map(str, X.dtypes)))\n",
    "    for k,dtype in dtypes:\n",
    "        input_els.append(Input(shape=(1,)))\n",
    "        if dtype == \"int8\":\n",
    "            e = Flatten()(Embedding(df[k].max()+1, 1)(input_els[-1]))\n",
    "        else:\n",
    "            e = input_els[-1]\n",
    "        encoded_els.append(e)\n",
    "    encoded_els = concatenate(encoded_els)\n",
    "\n",
    "    layer1 = Dropout(0.5)(Dense(100, activation=\"relu\")(encoded_els))\n",
    "    out = Dense(1, activation='sigmoid')(layer1)\n",
    "\n",
    "    # train model\n",
    "    model = Model(inputs=input_els, outputs=[out])\n",
    "    model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def f_in(X, m=None):\n",
    "    \"\"\"Preprocess input so it can be provided to a function\"\"\"\n",
    "    if m:\n",
    "        return [X.iloc[:m,i] for i in range(X.shape[1])]\n",
    "    else:\n",
    "        return [X.iloc[:,i] for i in range(X.shape[1])]\n",
    "\n",
    "def f_out(probs):\n",
    "    \"\"\"Convert probabilities into classes\"\"\"\n",
    "    return list((probs >= 0.5).astype(int).T[0])\n",
    "\n",
    "def plot_roc(y, probs):\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y, probs)\n",
    "\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(roc_auc)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.rcParams.update(params)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in list(np.logspace(0.6,4,dtype='int')):\n",
    "        if m >= len(X_train): break\n",
    "        model.fit(f_in(X_train,m), y_train[:m], epochs=50, batch_size=512, verbose=0)\n",
    "        y_train_predict = model.predict(f_in(X_train,m))\n",
    "        y_val_predict = model.predict(f_in(X_val))\n",
    "        y_train_predict = f_out(y_train_predict)\n",
    "        y_val_predict = f_out(y_val_predict)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    \n",
    "def keras_score(self, X, y, **kwargs):\n",
    "    \"\"\" Scorer class for eli5 library on feature importance\"\"\"\n",
    "    input_test = [X[:,i] for i in range(X.shape[1])]\n",
    "    loss = self.evaluate(input_test, y)\n",
    "    if type(loss) is list:\n",
    "        # The first one is the error, the rest are metrics\n",
    "        return -loss[0]\n",
    "    return -loss\n",
    "\n",
    "class ModelWrapper():\n",
    "    \"\"\" Keras model wrapper to override the predict function\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def predict(self, X, **kwargs):\n",
    "        return self.model.predict([X.iloc[:,i] for i in range(X.shape[1])])\n",
    "\n",
    "def plot_all_features(X, plot_numeric=True, hist=True, dropna=False):\n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    cols = 5\n",
    "    rows = math.ceil(float(X.shape[1]) / cols)\n",
    "    for i, column in enumerate(X.columns):\n",
    "        ax = fig.add_subplot(rows, cols, i + 1)\n",
    "        ax.set_title(column)\n",
    "        if X.dtypes[column] == np.object:\n",
    "            X[column].value_counts().plot(kind=\"bar\", axes=ax)\n",
    "        elif plot_numeric:\n",
    "            if hist:\n",
    "                X[column].hist(axes=ax)\n",
    "                plt.xticks(rotation=\"vertical\")\n",
    "            else:\n",
    "                if dropna:\n",
    "                    X[column].dropna().plot()\n",
    "                else:\n",
    "                    X[column].plot()\n",
    "                    \n",
    "    plt.subplots_adjust(hspace=0.7, wspace=0.2)\n",
    "    \n",
    "def plot_dendogram(corr, X):\n",
    "    corr_condensed = hc.distance.squareform(1-corr)\n",
    "    z = hc.linkage(corr_condensed, method=\"average\")\n",
    "    fig = plt.figure(figsize=(16,5))\n",
    "    dendrogram = hc.dendrogram(\n",
    "        z, labels=X.columns, orientation=\"left\", leaf_font_size=16)\n",
    "    plt.show()\n",
    "    \n",
    "def shap_predict(X):\n",
    "    values = model.predict([X[:,i] for i in range(X.shape[1])]).flatten()\n",
    "    return values\n",
    "\n",
    "def lime_predict_proba(X):\n",
    "    values = model.predict([X[:,i] for i in range(X.shape[1])]).flatten()\n",
    "    prob_pairs = np.array([1-values, values]).T\n",
    "    return prob_pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">Explainability and Bias Evaluation<br> with Tensorflow</font></h1> \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<h1>Alejandro Saucedo</h1>\n",
    "<br>\n",
    "Chief Scientist, The Institute for Ethical AI & Machine Learning\n",
    "<br>\n",
    "Contributing researcher, IEEE Algorithmic Bias Considerations Standard\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[github.com/ethicalml/bias-analysis](github.com/EthicalML/explainability-and-bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">NEW Project has come in!</font></h1> \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Insurance company wants to automate loan approval process\n",
    "\n",
    "<br>\n",
    "\n",
    "#### They have a manual process where a domain expert goes through applicants\n",
    "They want to automate this as they get 1m requests per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">Business wants it NOW!</font></h1>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Or yesterday if possible...\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "They heard their competitor is using \"Machine Learning\" and business says we need to use that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">The team had a look at how this worked</font></h1>\n",
    "\n",
    "<img src=\"images/mlall.png\" style=\"width=100vw\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">The team asked for DATA</font></h1> \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Business gave them an excel sheet with 25 rows\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The team pushed back, and after a while they finally got a dataset with ~8000 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">And so it begun...</font></h1> \n",
    "\n",
    "<img src=\"images/copypasta.jpg\" style=\"height:50vh\">\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## The journey towards greatness..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_data, df_display = get_dataset_1()\n",
    "\n",
    "df_display.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = df_data.drop(label_column, axis=1).copy()\n",
    "y = df_data[label_column].astype(int).values.copy()\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = \\\n",
    "        train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 1 layer, 100 neurons model, with softmax (0-1 probabilities)\n",
    "model = build_model(X)\n",
    "\n",
    "model.fit(f_in(X_train), y_train, epochs=10,\n",
    "    batch_size=512, shuffle=True, validation_data=(f_in(X_valid), y_valid),\n",
    "    callbacks=[PlotLossesKeras()], verbose=0, validation_split=0.05,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(f_in(X_valid), y_valid, verbose=1)\n",
    "print(\"Error %.4f: \" % score[0])\n",
    "print(\"Accuracy %.4f: \" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">Accuracy is 99%!</font></h1> \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### What a better result on a Friday evening!\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "##### Press the PROD button?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">2am emergency call...</font></h1> \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/layer.jpg\" style=\"height: 50vh\">\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "# ...what do you mean performing terrible? We followed the instructions!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">Time to diagnose!</font></h1> \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### We collect data visualised from production...\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "##### ...and label it to assess performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_prod, y_prod = get_production_dataset()\n",
    "\n",
    "X_prod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(f_in(X_prod), y_prod, verbose=1)\n",
    "probabilities = model.predict(f_in(X_prod))\n",
    "pred = f_out(probabilities)\n",
    "print(\"Accuracy %.4f: \" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "confusion = sklearn.metrics.confusion_matrix(y_prod, pred)\n",
    "confusion_df = pd.DataFrame(confusion,\n",
    "            index=[\"Actual Denied\", \"Actual Approved\"], \n",
    "            columns=[\"Preditced Denied\", \"Preditced Approved\"])\n",
    "\n",
    "sn.heatmap(confusion_df, annot=True, fmt='d', center=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_roc(y_prod, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(15,7))\n",
    "sn.countplot(y_valid, ax=ax[0]).set_xticklabels([\"male\", \"female\"])\n",
    "sn.countplot(y_prod, ax=ax[1]).set_xticklabels([\"male\", \"female\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">Undesired bias and explainability</font></h1>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* Has become popular due to several high profile incidents:\n",
    "\n",
    "    * Amazon's \"sexist\" recruitment tool\n",
    "    * Microsoft's \"racist\" chatbot\n",
    "    * Negative discrimination in automated sentencing\n",
    "    * Black box models with complex patterns that can't be interpretable\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Organisations cannot take on unknown risks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">This challenge goes beyond the algorithms</font></h1>\n",
    "\n",
    "### A large ethical decision should not just fall on the shoulders of a single data scientist\n",
    "\n",
    "<img src=\"images/chart.png\" style=\"height:30vw;margin-left: 10vw; float: left; background-color: transparent\">\n",
    "<img src=\"images/chart-more.png\" style=\"height:30vw; margin-left: 10vw; float: left;  background-color: transparent\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">The answer is not just about \"removing bias\"</font></h1>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* Any non trivial decision (i.e. more than 1 option) holds a bias, without exceptions.\n",
    "* It's impossible to \"just remove bias\", as the whole purpose of ML is to discriminate towards the right answer\n",
    "* Societal bias carries an inherent bias - what may be \"racist\" for one person, may not be for another group or geography\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## Let's see what \"undesired bias\" looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">Errors in scope (Bias a-priori)</font></h1>\n",
    "\n",
    "* Sub-optimal business objectives\n",
    "\n",
    "<br>\n",
    "\n",
    "* Lack of understanding of the project \n",
    "\n",
    "<br>\n",
    "\n",
    "* Incomplete resources (data, time, domain experts, etc)\n",
    "\n",
    "<br>\n",
    "\n",
    "* Incorrectly labelled data (accident vs otherwise)\n",
    "\n",
    "<br>\n",
    "\n",
    "* Lack of relevant skillset\n",
    "\n",
    "<br>\n",
    "\n",
    "* Societal shifts in perception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">Errors in project decisions (Bias a-posteriori)</font></h1>\n",
    "\n",
    "* Sub-optimal choices of accuracy metrics / cost functions\n",
    "\n",
    "<br>\n",
    "\n",
    "* Sub-optimal machine learning models chosen for the task \n",
    "\n",
    "<br>\n",
    "\n",
    "* Lack of infrastructure or metrics required to monitor model performance in production\n",
    "\n",
    "<br>\n",
    "\n",
    "* Lack of human-in-the-loop where necessary\n",
    "\n",
    "<br>\n",
    "\n",
    "* Not using resources at disposal (e.g. domain experts, tools, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">Tackling undesired bias through tools and humans</font></h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Focusing on process and best practices\n",
    "\n",
    "* In cybersecurity it's impossible to avoid all hacking attacks, but best practices help mitigate\n",
    "* Similarly, it's impossible to avoid all undesired bias, but it's possible to mitigate by having the right experts at the right touchpoints\n",
    "\n",
    "<br>\n",
    "\n",
    "## Using the right tools with the right domain experts\n",
    "\n",
    "* There are numerous techniques at the disposal of data scientists to analyse datasets and evaluate models\n",
    "* These should be use in conjunction with domain experts to understand explanatory/predictive modelling\n",
    "\n",
    "<br>\n",
    "\n",
    "## Ensure humans are leveraged\n",
    "\n",
    "* In some use-cases the level of scrutiny is higher due to the critical nature of the project\n",
    "* In those cases, the level of human review may be higher, and it may be necessary to have human-in-the-loop processes\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">How this looks in practice</font></h1>\n",
    "\n",
    "<img src=\"images/mlall.png\" style=\"width=100vw\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">Defining undesired bias through three processes</font></h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Data analysis\n",
    "\n",
    "* Class imbalances\n",
    "* Protected features\n",
    "* Correlations\n",
    "* Data representability\n",
    "\n",
    "<br>\n",
    "\n",
    "## Model Analysis\n",
    "\n",
    "* Feature importance analysis\n",
    "* Model specific explainability methods\n",
    "* Domain knowledge abstraction \n",
    "* Model metrics analysis\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "## Production monitoring\n",
    "\n",
    "* Thresholds set for evaluation metrics \n",
    "* Manual human review on critical decisions \n",
    "* Monitoring of anomalies and out-of-range predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">Augmenting the data science workflow</font></h1>\n",
    "\n",
    "<img src=\"images/gml.png\" style=\"width=100vw\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's see it in practice\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"images/xai.png\" style=\"width=100vw\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install xai\n",
    "import xai\n",
    "print(\"Loaded XAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X, y, X_train, X_valid, y_train, y_valid, X_display, y_display, df, df_display \\\n",
    "    = get_dataset_2()\n",
    "df_display.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "im = xai.show_imbalance(df_display, \"gender\", threshold=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "im = xai.show_imbalance(df_display, \"gender\", cross=[\"loan\",], categorical_cols=[\"loan\", \"gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "im = xai.balance(df_display, \"gender\", cross=[\"loan\",], categorical_cols=[\"loan\", \"gender\"],\n",
    "                upsample=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "protected_features = [\"ethnicity\", \"age\", \"gender\"]\n",
    "im = xai.show_imbalances(df_display, protected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "corr = xai.correlations(df_display, include_categorical=True, figsize=[20,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "corr = xai.correlations(df_display, include_categorical=True, figsize=[20,10], plot_type=\"matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_balanced, y_train_balanced, X_valid_balanced, y_valid_balanced = \\\n",
    "    xai.balanced_train_test_split(\n",
    "            X, y, cross=[\"gender\"], \n",
    "            categorical_cols=[\"gender\", \"loan\"], min_per_class=300)\n",
    "\n",
    "X_valid_balanced[\"loan\"] = y_valid_balanced\n",
    "im = xai.show_imbalance(X_valid_balanced, \"gender\", cross=[\"loan\"], categorical_cols=[\"gender\", \"loan\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 1 layer, 100 neurons model, with softmax\n",
    "model = build_model(X)\n",
    "model.fit(f_in(X_train), y_train, epochs=50, batch_size=512, shuffle=True, validation_data=(f_in(X_valid), y_valid), callbacks=[PlotLossesKeras()], verbose=0, validation_split=0.05,)\n",
    "probabilities = model.predict(f_in(X_valid))\n",
    "pred = f_out(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "confusion = sklearn.metrics.confusion_matrix(y_valid, pred)\n",
    "confusion_scaled = confusion.astype(\"float\") / confusion.sum(axis=1)[:, np.newaxis]\n",
    "confusion_scaled_df = pd.DataFrame(confusion_scaled, index=[\"Denied\", \"Approved\"], columns=[\"Denied\", \"Approved\"])\n",
    "sn.heatmap(confusion_scaled_df, annot=True, fmt='.2f', center=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "im = xai.roc_imbalance(X_valid, y_valid, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "im = xai.roc_imbalance(X_valid, y_valid, pred, col_name=\"gender\", categorical_cols=[\"gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = xai.metrics_imbalance(X_valid, y_valid, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = xai.metrics_imbalance(X_valid, y_valid, pred, col_name=\"gender\", categorical_cols=\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "imp = xai.feature_importance(X_valid, y_valid, lambda x, y: model.evaluate(f_in(x), y, verbose=0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "shap_explainer = shap.KernelExplainer(shap_predict, X.iloc[:100,:])\n",
    "shap_idx = 0\n",
    "shap_x = X.iloc[shap_idx,:]\n",
    "shap_display_x = X_display.iloc[shap_idx,:]\n",
    "shap_values = shap_explainer.shap_values(shap_x, nsamples=500)\n",
    "shap.force_plot(shap_explainer.expected_value, shap_values, shap_display_x, matplotlib=True, figsize=(40, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tf_lime_explainer = LimeTabularExplainer(X_train.values,\n",
    "                    feature_names=list(X_train.columns),\n",
    "                    categorical_features=categorical_features)\n",
    "\n",
    "tf_lime_explanation = tf_lime_explainer.explain_instance(\n",
    "        X_train.iloc[1,:], lime_predict_proba, num_features=13) \n",
    "\n",
    "tf_lime_explanation.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = xai.smile_imbalance(y_valid, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = xai.smile_imbalance(y_valid, probabilities, display_breakdown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = xai.smile_imbalance(y_valid, probabilities, threshold=0.67, manual_review=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">Revisiting our workflow</font></h1>\n",
    "\n",
    "<img src=\"images/gml.png\" style=\"width=100vw\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">The Explainability Tradeoff</font></h1>\n",
    "\n",
    "* Introducing fail-safe mechanisms, removing features, and using simpler models may have an impact on accuracy\n",
    "\n",
    "<br>\n",
    "\n",
    "* Not all usecases demand the same level of scrutiny\n",
    "\n",
    "<br>\n",
    "\n",
    "* The ones that are more critical do require a more strict process\n",
    "\n",
    "<br>\n",
    "\n",
    "* Similar to enterprise software, the overhead to offer accountability and governance is introduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<h1 style=\"font-size: 3em; line-height:2em;\"><font style=\"color: white !important;\">Explainability and Bias Evaluation<br> with Tensorflow</font></h1> \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<h1>Alejandro Saucedo</h1>\n",
    "<br>\n",
    "Chief Scientist, The Institute for Ethical AI & Machine Learning\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "[github.com/ethicalml/bias-analysis](github.com/ethicalml/bias-analysis)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
